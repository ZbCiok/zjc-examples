{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10632326,"sourceType":"datasetVersion","datasetId":6582717}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8, \n        #f2f4f4 20%, \n        #ffe5b4 40%, \n        #ffffcc 60%, \n        #d1f2eb 80%, \n        #f3e5f5 100%\n    );\n    padding: 20px; \n    margin: 20px 0; \n    border-radius: 10px; \n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    font-family: Arial, sans-serif; \n    color: #333;\n    text-align: center;\n\">\n<h1><strong>Bank Transaction Fraud Detection</strong></h1>\n<h2><strong>Problem Statement</strong></h2>\n<p>With the rapid growth of digital banking, fraudulent transactions have become a significant concern for financial institutions. The challenge is to build a robust system to detect and prevent fraudulent transactions in real-time while maintaining customer convenience and privacy.</p>\n<p>The dataset provided contains detailed information about bank transactions, including customer demographics, transaction metadata, merchant categories, device types, transaction locations, and other relevant attributes. Key fields like transaction descriptions, device usage, and merchant categories provide vital insights for identifying anomalous activities. The \"Is_Fraud\" label offers a foundation for supervised learning techniques to differentiate between genuine and fraudulent transactions.</p>\n<p>The objective of this problem is to analyze transaction patterns and develop predictive models that can accurately classify transactions as fraudulent or legitimate. This task involves exploring feature correlations, detecting unusual transaction behavior, and leveraging machine learning algorithms to create a scalable and efficient fraud detection system.</p>\n<p>A successful solution will not only detect fraudulent activities but also minimize false positives, ensuring genuine transactions are not unnecessarily flagged. Insights derived from this analysis can help strengthen security measures, optimize fraud prevention strategies, and enhance the overall banking experience for customers.</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8, \n        #f2f4f4 20%, \n        #ffe5b4 40%, \n        #ffffcc 60%, \n        #d1f2eb 80%, \n        #f3e5f5 100%\n    );\n    padding: 20px; \n    margin: 20px 0; \n    border-radius: 10px; \n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    font-family: Arial, sans-serif; \n    color: #333;\n    text-align: left;\n\">\n<h2><strong>Objectives for Bank Transaction Fraud Detection</strong></h2>\n<ol>\n<li>\n<p><strong>Fraud Detection:</strong></p>\n<ul>\n<li>Develop a predictive model to classify bank transactions as fraudulent or legitimate using historical transaction data.</li>\n</ul>\n</li>\n<li>\n<p><strong>Anomaly Detection:</strong></p>\n<ul>\n<li>Identify unusual patterns or behaviors in customer transactions that may indicate potential fraud.</li>\n</ul>\n</li>\n<li>\n<p><strong>Feature Analysis:</strong></p>\n<ul>\n<li>Explore key features such as merchant categories, transaction devices, transaction locations, and account types to understand their impact on fraud detection.</li>\n</ul>\n</li>\n<li>\n<p><strong>Model Performance Optimization:</strong></p>\n<ul>\n<li>Ensure the fraud detection system achieves high accuracy, precision, and recall while minimizing false positives and false negatives.</li>\n</ul>\n</li>\n<li>\n<p><strong>Real-Time Fraud Prevention:</strong></p>\n<ul>\n<li>Create a scalable solution that can potentially be adapted for real-time fraud detection in production environments.</li>\n</ul>\n</li>\n<li>\n<p><strong>Customer Behavior Insights:</strong></p>\n<ul>\n<li>Analyze legitimate transaction behaviors to gain insights into customer banking patterns and preferences.</li>\n</ul>\n</li>\n<li>\n<p><strong>Device and Location Security:</strong></p>\n<ul>\n<li>Understand the correlation between transaction device types, locations, and fraudulent activities.</li>\n</ul>\n</li>\n<li>\n<p><strong>Security Enhancements:</strong></p>\n<ul>\n<li>Provide actionable recommendations to the bank for improving fraud prevention strategies and enhancing digital transaction security.</li>\n</ul>\n</li>\n</ol>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 1 - Importing Libraries</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\n# from sklearn.linear_model import LinearDiscriminantAnalysis as LDA, RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import metrics\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nimport warnings\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:06.806242Z","iopub.execute_input":"2025-02-01T09:54:06.806548Z","iopub.status.idle":"2025-02-01T09:54:12.408029Z","shell.execute_reply.started":"2025-02-01T09:54:06.806523Z","shell.execute_reply":"2025-02-01T09:54:12.407327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:12.408818Z","iopub.execute_input":"2025-02-01T09:54:12.409453Z","iopub.status.idle":"2025-02-01T09:54:12.42021Z","shell.execute_reply.started":"2025-02-01T09:54:12.409428Z","shell.execute_reply":"2025-02-01T09:54:12.419542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv(file_path)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:12.421803Z","iopub.execute_input":"2025-02-01T09:54:12.422028Z","iopub.status.idle":"2025-02-01T09:54:14.495013Z","shell.execute_reply.started":"2025-02-01T09:54:12.422009Z","shell.execute_reply":"2025-02-01T09:54:14.494301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:14.496065Z","iopub.execute_input":"2025-02-01T09:54:14.496371Z","iopub.status.idle":"2025-02-01T09:54:14.689885Z","shell.execute_reply.started":"2025-02-01T09:54:14.496349Z","shell.execute_reply":"2025-02-01T09:54:14.68911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:14.690686Z","iopub.execute_input":"2025-02-01T09:54:14.690904Z","iopub.status.idle":"2025-02-01T09:54:14.695791Z","shell.execute_reply.started":"2025-02-01T09:54:14.690883Z","shell.execute_reply":"2025-02-01T09:54:14.69498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:14.696615Z","iopub.execute_input":"2025-02-01T09:54:14.69683Z","iopub.status.idle":"2025-02-01T09:54:15.08707Z","shell.execute_reply.started":"2025-02-01T09:54:14.696812Z","shell.execute_reply":"2025-02-01T09:54:15.086391Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 2 - Data Preprocessing</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Checking for missing values\nprint(\"Missing NULL values in the dataset:\")\nprint(df.isnull().sum())\nprint(\"-\"*80)\nprint(\"Missing N/A values in the dataset:\")\nprint(df.isna().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:15.087708Z","iopub.execute_input":"2025-02-01T09:54:15.087919Z","iopub.status.idle":"2025-02-01T09:54:15.450327Z","shell.execute_reply.started":"2025-02-01T09:54:15.087895Z","shell.execute_reply":"2025-02-01T09:54:15.449568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"desc = pd.DataFrame(index = list(df))\ndesc['type'] = df.dtypes\ndesc['count'] = df.count()\ndesc['nunique'] = df.nunique()\ndesc['%unique'] = desc['nunique'] /len(df) * 100\ndesc['null'] = df.isnull().sum()\ndesc['%null'] = desc['null'] / len(df) * 100\ndesc = pd.concat([desc,df.describe().T.drop('count',axis=1)],axis=1)\ndesc.sort_values(by=['type','null']).style.background_gradient(cmap='YlOrBr')\\\n    .bar(subset=['mean'],color='green')\\\n    .bar(subset=['max'],color='red')\\\n    .bar(subset=['min'], color='pink')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:15.451103Z","iopub.execute_input":"2025-02-01T09:54:15.451393Z","iopub.status.idle":"2025-02-01T09:54:16.291607Z","shell.execute_reply.started":"2025-02-01T09:54:15.451357Z","shell.execute_reply":"2025-02-01T09:54:16.290687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a list of categorical columns in the dataframe\ncategorical_columns = df.select_dtypes(include=['object']).columns\n\n# Check the unique values and their counts for each categorical column\nfor col in categorical_columns:\n    print(f\"Column: {col}\")\n    print(\"-\" * 25)\n    print(f\"Unique values: {df[col].nunique()}\")\n    print(f\"Unique values sample: {df[col].unique()[:10]}\")  # Display a sample of unique values\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:16.293978Z","iopub.execute_input":"2025-02-01T09:54:16.294245Z","iopub.status.idle":"2025-02-01T09:54:17.000163Z","shell.execute_reply.started":"2025-02-01T09:54:16.2942Z","shell.execute_reply":"2025-02-01T09:54:16.999513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If a column has only one unique value, it won't be useful for prediction.\nsingle_value_columns = [col for col in df.columns if df[col].nunique() == 1]\nprint(\"Columns with only one unique value:\", single_value_columns)\n\n# Dropping columns with one unique value\ndf = df.drop(columns=single_value_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:17.001848Z","iopub.execute_input":"2025-02-01T09:54:17.002184Z","iopub.status.idle":"2025-02-01T09:54:17.403089Z","shell.execute_reply.started":"2025-02-01T09:54:17.002148Z","shell.execute_reply":"2025-02-01T09:54:17.402162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking columns after dropping one unique columns\ndf.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:17.404043Z","iopub.execute_input":"2025-02-01T09:54:17.404384Z","iopub.status.idle":"2025-02-01T09:54:17.409526Z","shell.execute_reply.started":"2025-02-01T09:54:17.404353Z","shell.execute_reply":"2025-02-01T09:54:17.408672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop the columns which are not useful for the model evaluation\ndf = df.drop(columns=['Customer_Contact', 'Customer_Email', 'Customer_Name', 'Customer_ID', 'Transaction_ID', 'Merchant_ID'])\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:17.410474Z","iopub.execute_input":"2025-02-01T09:54:17.410771Z","iopub.status.idle":"2025-02-01T09:54:17.461068Z","shell.execute_reply.started":"2025-02-01T09:54:17.410742Z","shell.execute_reply":"2025-02-01T09:54:17.460392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking columns after dropping not useful columns\ndf.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:17.461822Z","iopub.execute_input":"2025-02-01T09:54:17.462039Z","iopub.status.idle":"2025-02-01T09:54:17.46659Z","shell.execute_reply.started":"2025-02-01T09:54:17.462022Z","shell.execute_reply":"2025-02-01T09:54:17.465958Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 3 - Exploratory Data Analysis (EDA)</strong></h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8, \n        #f2f4f4 20%, \n        #ffe5b4 40%, \n        #ffffcc 60%, \n        #d1f2eb 80%, \n        #f3e5f5 100%\n    );\n    padding: 20px; \n    margin: 20px 0; \n    border-radius: 10px; \n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    font-family: Arial, sans-serif; \n    color: #333;\n    text-align: center;\n\">\n    <h3><strong>EDA for Numerical Columns</strong></h3>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# For numerical columns, we'll fill missing values with the median of each column\nnumerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\nfor col in numerical_columns:\n    df[col] = df[col].fillna(df[col].median())\n\nprint(numerical_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:17.467427Z","iopub.execute_input":"2025-02-01T09:54:17.46762Z","iopub.status.idle":"2025-02-01T09:54:17.494859Z","shell.execute_reply.started":"2025-02-01T09:54:17.467603Z","shell.execute_reply":"2025-02-01T09:54:17.494258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a figure with 2 subplots in a horizontal row\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))  # 1 row, 2 columns\n\n# KDE plot for the 'Is_Fraud' column (on the first subplot)\nsns.kdeplot(df[\"Is_Fraud\"], fill=True, ax=axes[0])\naxes[0].set_title('Target Variable Distribution')\n\n# Count plot for the 'Is_Fraud' column (on the second subplot)\nsns.countplot(x='Is_Fraud', data=df, ax=axes[1])\naxes[1].set_title('Fraudulent Transactions Count')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:17.495494Z","iopub.execute_input":"2025-02-01T09:54:17.495682Z","iopub.status.idle":"2025-02-01T09:54:18.720068Z","shell.execute_reply.started":"2025-02-01T09:54:17.495665Z","shell.execute_reply":"2025-02-01T09:54:18.719108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loop through each numerical column in your DataFrame\nfor col in numerical_columns:\n    plt.style.use(\"fivethirtyeight\")\n    plt.figure(figsize=(10, 6))\n    \n    # Create the boxplot\n    sns.boxplot(x=df[col])\n    plt.title(f'Distribution of {col}')\n    plt.xlabel(col)\n    \n    # Show the plot\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:18.720838Z","iopub.execute_input":"2025-02-01T09:54:18.721062Z","iopub.status.idle":"2025-02-01T09:54:19.194407Z","shell.execute_reply.started":"2025-02-01T09:54:18.721042Z","shell.execute_reply":"2025-02-01T09:54:19.193558Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8, \n        #f2f4f4 20%, \n        #ffe5b4 40%, \n        #ffffcc 60%, \n        #d1f2eb 80%, \n        #f3e5f5 100%\n    );\n    padding: 20px; \n    margin: 20px 0; \n    border-radius: 10px; \n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    font-family: Arial, sans-serif; \n    color: #333;\n    text-align: center;\n\">\n    <h3><strong>EDA for Categorical Columns</strong></h3>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# For categorical columns, we'll fill missing values with the mode (most frequent category)\ncategorical_columns = df.select_dtypes(include=['object']).columns\nfor col in categorical_columns:\n    df[col] = df[col].fillna(df[col].mode()[0])\n    \nprint(categorical_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:19.195352Z","iopub.execute_input":"2025-02-01T09:54:19.195607Z","iopub.status.idle":"2025-02-01T09:54:19.543264Z","shell.execute_reply.started":"2025-02-01T09:54:19.195586Z","shell.execute_reply":"2025-02-01T09:54:19.542492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a figure with 4 subplots in a horizontal row\nfig, axes = plt.subplots(1, 2, figsize=(20, 6))  # 1 row, 4 columns\n\n# Histogram for the 'Age' column (on the third subplot)\nsns.histplot(df['Age'], kde=True, ax=axes[0], color='skyblue')\naxes[0].set_title('Age Distribution')\n\n# Count plot for the 'Gender' column (on the fourth subplot)\nsns.countplot(x='Gender', data=df, ax=axes[1])\naxes[1].set_title('Gender Distribution')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:19.544071Z","iopub.execute_input":"2025-02-01T09:54:19.54439Z","iopub.status.idle":"2025-02-01T09:54:20.902166Z","shell.execute_reply.started":"2025-02-01T09:54:19.544358Z","shell.execute_reply":"2025-02-01T09:54:20.901321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the number of rows needed based on the number of charts\nnum_cols = 3  # Number of charts per row\n# num_rows = (len(categorical_columns) + num_cols - 1) // num_cols  # Calculate rows required for all charts\nnum_rows = 2 # Number of rows\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 6))  # Adjust figure size for more rows\n\n# Flatten the axes array for easier iteration\naxes = axes.flatten()\n\nax_index = 0\nfor col in categorical_columns:\n    unique_values = df[col].nunique()\n    if unique_values < 10:  # Only plot if unique values are less than 10\n        # Plot on the respective subplot\n        ax = axes[ax_index]\n        ax.pie(df[col].value_counts(), labels=df[col].unique(), autopct='%1.1f%%')\n        ax.set_title(f'{col} Distribution')\n        \n        # Move to the next subplot\n        ax_index += 1\n\n# Hide any unused subplots (in case there are fewer than `num_rows * num_cols` charts)\nfor i in range(ax_index, len(axes)):\n    axes[i].axis('off')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:20.903018Z","iopub.execute_input":"2025-02-01T09:54:20.903358Z","iopub.status.idle":"2025-02-01T09:54:21.753657Z","shell.execute_reply.started":"2025-02-01T09:54:20.903335Z","shell.execute_reply":"2025-02-01T09:54:21.752767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter categorical columns with less than 20 unique values\ncategorical_cols = df.select_dtypes(include=['object']).columns\ncategorical_cols = [col for col in categorical_cols if df[col].nunique() < 20]\n\n# Set the number of charts per row and rows\nnum_cols = 3  # Number of charts per row\nnum_rows = 2  # Number of rows\n\n# Calculate the total number of subplots needed\ntotal_plots = len(categorical_cols)\n\n# Create a figure with the appropriate number of rows and columns\nplt.figure(figsize=(15, 5 * num_rows))\n\n# Plot the count plots for the filtered categorical columns\nfor i, col in enumerate(categorical_cols):\n    plt.subplot(num_rows, num_cols, i + 1)\n    sns.countplot(data=df, x=col, hue='Is_Fraud') \n    plt.title(f'Fraud by {col}')\n    plt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate churn rate by categories\nprint(\"\\Fraud Rate by Categories:\")\nfor col in categorical_cols:\n    print(f\"\\n{col} Analysis:\")\n    print(df.groupby(col)['Is_Fraud'].mean().round(3) * 100)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:21.754396Z","iopub.execute_input":"2025-02-01T09:54:21.754611Z","iopub.status.idle":"2025-02-01T09:54:23.811256Z","shell.execute_reply.started":"2025-02-01T09:54:21.754593Z","shell.execute_reply":"2025-02-01T09:54:23.810485Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 4 - Convert Date Time Columns to Numerical Columns</strong></h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Convert 'Transaction_Date' and 'Transaction_Time' to datetime","metadata":{}},{"cell_type":"code","source":"df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'], format='%d-%m-%Y')\ndf['Transaction_Time'] = pd.to_datetime(df['Transaction_Time'], format='%H:%M:%S')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:23.81206Z","iopub.execute_input":"2025-02-01T09:54:23.812381Z","iopub.status.idle":"2025-02-01T09:54:24.28264Z","shell.execute_reply.started":"2025-02-01T09:54:23.812351Z","shell.execute_reply":"2025-02-01T09:54:24.28184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract new features from 'Transaction_Date' and 'Transaction_Time'\ndf['Transaction_Day'] = df['Transaction_Date'].dt.day\ndf['Transaction_Month'] = df['Transaction_Date'].dt.month\ndf['Transaction_Year'] = df['Transaction_Date'].dt.year\ndf['Transaction_Hour'] = df['Transaction_Time'].dt.hour\ndf['Transaction_Minute'] = df['Transaction_Time'].dt.minute\ndf['Transaction_Second'] = df['Transaction_Time'].dt.second","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:24.283492Z","iopub.execute_input":"2025-02-01T09:54:24.283773Z","iopub.status.idle":"2025-02-01T09:54:24.31902Z","shell.execute_reply.started":"2025-02-01T09:54:24.283739Z","shell.execute_reply":"2025-02-01T09:54:24.318411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop 'Transaction_Date' and 'Transaction_Time' columns after feature extraction\ndf = df.drop(columns=['Transaction_Date', 'Transaction_Time'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:24.31978Z","iopub.execute_input":"2025-02-01T09:54:24.320012Z","iopub.status.idle":"2025-02-01T09:54:24.348266Z","shell.execute_reply.started":"2025-02-01T09:54:24.319993Z","shell.execute_reply":"2025-02-01T09:54:24.347557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If a column has only one unique value, it won't be useful for prediction.\nsingle_value_cols = [col for col in df.columns if df[col].nunique() == 1]\nprint(\"Columns with only one unique value:\", single_value_columns)\n\n# Dropping columns with one unique value\ndf = df.drop(columns=single_value_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:24.348902Z","iopub.execute_input":"2025-02-01T09:54:24.349118Z","iopub.status.idle":"2025-02-01T09:54:24.526296Z","shell.execute_reply.started":"2025-02-01T09:54:24.3491Z","shell.execute_reply":"2025-02-01T09:54:24.525533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For numerical columns, updating after conversion\nnumerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\nprint(\"Numerical Columns ::\", numerical_columns)\nprint(\"-\"*50)\n# For categorical columns, updating after conversion\ncategorical_columns = df.select_dtypes(include=['object']).columns\nprint(\"Categorical Columns ::\", categorical_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:24.527049Z","iopub.execute_input":"2025-02-01T09:54:24.527303Z","iopub.status.idle":"2025-02-01T09:54:24.563995Z","shell.execute_reply.started":"2025-02-01T09:54:24.527283Z","shell.execute_reply":"2025-02-01T09:54:24.563189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:24.564763Z","iopub.execute_input":"2025-02-01T09:54:24.564978Z","iopub.status.idle":"2025-02-01T09:54:24.580118Z","shell.execute_reply.started":"2025-02-01T09:54:24.56496Z","shell.execute_reply":"2025-02-01T09:54:24.579425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 5 - Encode Categorical Features</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Initializing the LabelEncoder\nlabel_encoder = LabelEncoder()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:24.584336Z","iopub.execute_input":"2025-02-01T09:54:24.584567Z","iopub.status.idle":"2025-02-01T09:54:24.594456Z","shell.execute_reply.started":"2025-02-01T09:54:24.584548Z","shell.execute_reply":"2025-02-01T09:54:24.593644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in categorical_columns:\n    df[col] = label_encoder.fit_transform(df[col])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:24.596028Z","iopub.execute_input":"2025-02-01T09:54:24.596277Z","iopub.status.idle":"2025-02-01T09:54:25.019455Z","shell.execute_reply.started":"2025-02-01T09:54:24.596247Z","shell.execute_reply":"2025-02-01T09:54:25.018627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:25.020298Z","iopub.execute_input":"2025-02-01T09:54:25.020529Z","iopub.status.idle":"2025-02-01T09:54:25.034988Z","shell.execute_reply.started":"2025-02-01T09:54:25.020509Z","shell.execute_reply":"2025-02-01T09:54:25.034119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:25.035769Z","iopub.execute_input":"2025-02-01T09:54:25.036038Z","iopub.status.idle":"2025-02-01T09:54:25.065548Z","shell.execute_reply.started":"2025-02-01T09:54:25.036014Z","shell.execute_reply":"2025-02-01T09:54:25.064728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:25.066356Z","iopub.execute_input":"2025-02-01T09:54:25.066595Z","iopub.status.idle":"2025-02-01T09:54:25.11607Z","shell.execute_reply.started":"2025-02-01T09:54:25.066574Z","shell.execute_reply":"2025-02-01T09:54:25.115333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 6 - EDA after Label Encoder</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter numerical columns with less than 20 unique values\nnumerical_features = df.select_dtypes(include=['float64', 'int64']).columns\nnumerical_features = [col for col in numerical_features if df[col].nunique() < 200]\n\n# Set the number of charts per row\nnum_cols = 2  # Number of charts per row\n\n# Calculate the number of rows needed based on the number of features\nnum_rows = (len(numerical_features) + num_cols - 1) // num_cols  # This ensures enough rows are created\n\n# Create a figure with the appropriate number of rows and columns\nplt.figure(figsize=(15, 5 * num_rows))\n\n# Plot the histograms for the filtered numerical columns\nfor i, feature in enumerate(numerical_features):\n    plt.subplot(num_rows, num_cols, i + 1)\n    sns.histplot(data=df, x=feature, hue='Is_Fraud', bins=30)\n    plt.title(f'{feature} Distribution by Churn Status')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:25.11683Z","iopub.execute_input":"2025-02-01T09:54:25.117092Z","iopub.status.idle":"2025-02-01T09:54:30.743091Z","shell.execute_reply.started":"2025-02-01T09:54:25.117069Z","shell.execute_reply":"2025-02-01T09:54:30.741974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 7 - Visualize Fraud Patterns and Distribution of Features</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Create a figure with 2 subplots in a horizontal row\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))  # 1 row, 2 columns\n\n# KDE plot for the 'Is_Fraud' column (on the first subplot)\nsns.kdeplot(df[\"Is_Fraud\"], fill=True, ax=axes[0])\naxes[0].set_title('Target Variable Distribution')\n\n# Count plot for the 'Is_Fraud' column (on the second subplot)\nsns.countplot(x='Is_Fraud', data=df, ax=axes[1])\naxes[1].set_title('Fraudulent Transactions Count')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:30.74399Z","iopub.execute_input":"2025-02-01T09:54:30.744264Z","iopub.status.idle":"2025-02-01T09:54:32.011338Z","shell.execute_reply.started":"2025-02-01T09:54:30.744214Z","shell.execute_reply":"2025-02-01T09:54:32.010416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize fraud transactions based on 'Transaction_Amount'\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Is_Fraud', y='Transaction_Amount', data=df)\nplt.title(\"Transaction Amount vs Fraud/Non-Fraud\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:32.012295Z","iopub.execute_input":"2025-02-01T09:54:32.01256Z","iopub.status.idle":"2025-02-01T09:54:32.19315Z","shell.execute_reply.started":"2025-02-01T09:54:32.012532Z","shell.execute_reply":"2025-02-01T09:54:32.192319Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 8 - Plot Correlation Matrix to Understand Feature Relationships</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14, 10))\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:32.193895Z","iopub.execute_input":"2025-02-01T09:54:32.19416Z","iopub.status.idle":"2025-02-01T09:54:33.543818Z","shell.execute_reply.started":"2025-02-01T09:54:32.194139Z","shell.execute_reply":"2025-02-01T09:54:33.542688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate correlation matrix for numerical columns\ncorrelation_matrix = df.corr()\n\n# Extract correlation with 'Exited' and drop 'Exited' itself\ncorrelation_price = correlation_matrix['Is_Fraud'].sort_values(ascending=False).drop('Is_Fraud')\n\n# Plot the heatmap for the correlation with 'Exited'\nplt.figure(figsize=(8, 5))\nsns.heatmap(correlation_price.to_frame(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Correlation with Exited')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:33.54477Z","iopub.execute_input":"2025-02-01T09:54:33.545058Z","iopub.status.idle":"2025-02-01T09:54:34.068423Z","shell.execute_reply.started":"2025-02-01T09:54:33.545032Z","shell.execute_reply":"2025-02-01T09:54:34.067624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 9 - Feature Importance using Random Forest</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=100, random_state=42)\nX = df.drop(columns=['Is_Fraud'])\ny = df['Is_Fraud']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:34.069283Z","iopub.execute_input":"2025-02-01T09:54:34.069495Z","iopub.status.idle":"2025-02-01T09:54:34.08501Z","shell.execute_reply.started":"2025-02-01T09:54:34.069477Z","shell.execute_reply":"2025-02-01T09:54:34.084159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Shape for X Dataframe: \", X.shape)\nprint(\"Columns for X Dataframe: \", X.columns)\nprint(\"-\"*50)\nprint(\"Shape for y Dataframe: \", y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:34.086079Z","iopub.execute_input":"2025-02-01T09:54:34.086372Z","iopub.status.idle":"2025-02-01T09:54:34.092147Z","shell.execute_reply.started":"2025-02-01T09:54:34.086352Z","shell.execute_reply":"2025-02-01T09:54:34.091538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nrf.fit(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:54:34.092831Z","iopub.execute_input":"2025-02-01T09:54:34.09304Z","iopub.status.idle":"2025-02-01T09:55:30.998365Z","shell.execute_reply.started":"2025-02-01T09:54:34.093021Z","shell.execute_reply":"2025-02-01T09:55:30.997628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get feature importances\nfeature_importances = pd.DataFrame(rf.feature_importances_, index=X.columns, columns=['importance'])\nfeature_importances = feature_importances.sort_values('importance', ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:55:30.999092Z","iopub.execute_input":"2025-02-01T09:55:30.999384Z","iopub.status.idle":"2025-02-01T09:55:31.038149Z","shell.execute_reply.started":"2025-02-01T09:55:30.999351Z","shell.execute_reply":"2025-02-01T09:55:31.037298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot feature importances\nplt.figure(figsize=(12, 8))\nfeature_importances.head(20).plot(kind='bar', figsize=(10, 6))\nplt.title(\"Top 20 Feature Importances\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:55:31.039017Z","iopub.execute_input":"2025-02-01T09:55:31.039347Z","iopub.status.idle":"2025-02-01T09:55:31.350303Z","shell.execute_reply.started":"2025-02-01T09:55:31.039324Z","shell.execute_reply":"2025-02-01T09:55:31.3495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 10 - Select Only Important Features</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Select features with importance greater than a threshold (e.g., 0.01)\nimportant_features = feature_importances[feature_importances['importance'] > 0.01].index\nX = df[important_features]\nprint(\"Shape for X Dataframe: \", X.shape)\nprint(\"Columns for X Dataframe: \", X.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:55:31.351185Z","iopub.execute_input":"2025-02-01T09:55:31.351482Z","iopub.status.idle":"2025-02-01T09:55:31.370718Z","shell.execute_reply.started":"2025-02-01T09:55:31.35145Z","shell.execute_reply":"2025-02-01T09:55:31.369956Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 11 - Perform PCA (Principal Component Analysis)</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# If the number of features is large, PCA can help reduce dimensions\npca = PCA(n_components=2)  # Reducing to 2 components for visualization\nX_pca = pca.fit_transform(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:55:31.371624Z","iopub.execute_input":"2025-02-01T09:55:31.371912Z","iopub.status.idle":"2025-02-01T09:55:31.70151Z","shell.execute_reply.started":"2025-02-01T09:55:31.371884Z","shell.execute_reply":"2025-02-01T09:55:31.700779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot PCA results\nplt.figure(figsize=(8, 6))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='coolwarm')\nplt.title(\"PCA of Important Features\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.colorbar(label='Fraud (1) vs Non-Fraud (0)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:55:31.702287Z","iopub.execute_input":"2025-02-01T09:55:31.702593Z","iopub.status.idle":"2025-02-01T09:55:35.105136Z","shell.execute_reply.started":"2025-02-01T09:55:31.702564Z","shell.execute_reply":"2025-02-01T09:55:35.10431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 12 - Train-test Split</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:55:35.106033Z","iopub.execute_input":"2025-02-01T09:55:35.106314Z","iopub.status.idle":"2025-02-01T09:55:35.167293Z","shell.execute_reply.started":"2025-02-01T09:55:35.106293Z","shell.execute_reply":"2025-02-01T09:55:35.166184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 13 - Feature Scaling</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:55:35.1686Z","iopub.execute_input":"2025-02-01T09:55:35.168872Z","iopub.status.idle":"2025-02-01T09:55:35.248972Z","shell.execute_reply.started":"2025-02-01T09:55:35.168852Z","shell.execute_reply":"2025-02-01T09:55:35.24803Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 14 - Model Training and Evaluation</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Define models\nmodels = {\n    'Logistic Regression': LogisticRegression(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'Gradient Boosting': GradientBoostingClassifier(),\n    'XGBoost': xgb.XGBClassifier(),\n    'LightGBM': lgb.LGBMClassifier(),\n    'CatBoost': cb.CatBoostClassifier(silent=True),\n    'AdaBoost': AdaBoostClassifier(),\n    'Bagging': BaggingClassifier(),\n    'KNN': KNeighborsClassifier()\n    # 'SVM (RBF)': SVC(kernel='rbf', probability=True),\n    # 'SVM (Linear)': LinearSVC(),\n    # 'GaussianNB': GaussianNB()\n    # 'LDA': LDA(),\n    # 'QDA': QuadraticDiscriminantAnalysis(),\n    # 'Ridge Classifier': RidgeClassifier(),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:55:55.357743Z","iopub.execute_input":"2025-02-01T09:55:55.358043Z","iopub.status.idle":"2025-02-01T09:55:55.369649Z","shell.execute_reply.started":"2025-02-01T09:55:55.35802Z","shell.execute_reply":"2025-02-01T09:55:55.368944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define reduced parameter grids\nparam_grids = {\n    'Logistic Regression': {\n        'C': [0.1, 1],\n        'solver': ['liblinear'],\n        'penalty': ['l2']\n    },\n    'Decision Tree': {\n        'max_depth': [5, 10],\n        'min_samples_split': [2, 5],\n        'min_samples_leaf': [1, 2]\n    },\n    'Random Forest': {\n        'n_estimators': [50, 100],\n        'max_depth': [10],\n        'min_samples_split': [2],\n        'min_samples_leaf': [1]\n    },\n    'Gradient Boosting': {\n        'n_estimators': [100],\n        'learning_rate': [0.1],\n        'max_depth': [5]\n    },\n    'XGBoost': {\n        'n_estimators': [100],\n        'learning_rate': [0.1],\n        'max_depth': [5],\n        'subsample': [0.8, 1.0]\n    },\n    'SVM (RBF)': {\n        'C': [1, 10],\n        'gamma': ['scale', 'auto']\n    },\n    'SVM (Linear)': {\n        'C': [1, 10],\n    },\n    'LightGBM': {\n        'n_estimators': [100],\n        'learning_rate': [0.1],\n        'max_depth': [3, 5],\n    },\n    'CatBoost': {\n        'iterations': [100],\n        'learning_rate': [0.1],\n        'depth': [3, 5]\n    },\n    'KNN': {\n        'n_neighbors': [3],\n        'weights': ['uniform', 'distance']\n    },\n    'AdaBoost': {\n        'n_estimators': [100],\n        'learning_rate': [0.01, 0.1]\n    },\n    'Bagging': {\n        'n_estimators': [100],\n        'max_samples': [0.8, 1.0]\n    },\n    'LDA': {},\n    'QDA': {},\n    'Ridge Classifier': {\n        'alpha': [0.1, 1]\n    },\n    'GaussianNB': {}\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:56:13.277164Z","iopub.execute_input":"2025-02-01T09:56:13.277515Z","iopub.status.idle":"2025-02-01T09:56:13.284207Z","shell.execute_reply.started":"2025-02-01T09:56:13.27749Z","shell.execute_reply":"2025-02-01T09:56:13.283293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize an empty dictionary to store results\nmodel_results = {}\n\n# Handle class imbalance by computing class weights for each model that supports it\nclass_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train)\nclass_weight_dict = {0: class_weights[0], 1: class_weights[1]}\nprint(\"class_weight_dict: \", class_weight_dict)\n\n# Handle SMOTE for class imbalance\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n\n# Evaluate models with GridSearchCV\nfor model_name, model in models.items():\n    print(f\"Training model with GridSearchCV: {model_name}\")\n    \n    # Get the parameter grid for the model\n    param_grid = param_grids[model_name]\n    \n    # Modify model to include class weights where applicable\n    if model_name in ['Logistic Regression', 'Random Forest', 'SVM (RBF)', 'SVM (Linear)']:\n        # Assign class weights for models that support it\n        if model_name == 'Logistic Regression':\n            model = LogisticRegression(class_weight='balanced')\n        elif model_name == 'Random Forest':\n            model = RandomForestClassifier(class_weight='balanced')\n        elif model_name in ['SVM (RBF)', 'SVM (Linear)']:\n            model = SVC(probability=True, class_weight='balanced') if model_name == 'SVM (RBF)' else LinearSVC(class_weight='balanced')\n\n    # Perform GridSearchCV with parallelism\n    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n    \n    # Fit the model with the best parameters using the resampled data\n    grid_search.fit(X_train_smote, y_train_smote)\n    \n    # Get the best model and its parameters\n    best_model = grid_search.best_estimator_\n    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n    \n    # Predict on both train and test sets\n    y_train_pred = best_model.predict(X_train_smote)\n    y_test_pred = best_model.predict(X_test_scaled)\n    \n    # Store the results\n    model_results[model_name] = {\n        'train_accuracy': best_model.score(X_train_smote, y_train_smote),\n        'test_accuracy': best_model.score(X_test_scaled, y_test),\n        'y_test': y_test,\n        'y_test_pred': y_test_pred,\n        'classification_report': classification_report(y_test, y_test_pred),\n        'roc_auc': roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n    }\n\n    # Print results after all models are evaluated\n    print(\"\\nModel Evaluation Results:\")\n    print(f\"Model: {model_results[model_name]}\\n\")\n    print(f\"Train Accuracy: {model_results[model_name]['train_accuracy']:.4f}\")\n    print(f\"Test Accuracy: {model_results[model_name]['test_accuracy']:.4f}\")\n    print(f\"ROC AUC: {model_results[model_name]['roc_auc']:.4f}\\n\")\n    print(f\"Classification Report:\\n{model_results[model_name]['classification_report']}\")\n    print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:56:30.342928Z","iopub.execute_input":"2025-02-01T09:56:30.343284Z","execution_failed":"2025-02-01T10:39:12.836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 15 - Displaying Evaluation Results for All Models</strong></h2>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# # Print results after all models are evaluated\n# print(\"\\nModel Evaluation Results:\")\n# print(f\"Model: {model_results[model_name]}\\n\")\n# print(f\"Train Accuracy: {model_results[model_name]['train_accuracy']:.4f}\")\n# print(f\"Test Accuracy: {model_results[model_name]['test_accuracy']:.4f}\")\n# print(f\"ROC AUC: {model_results[model_name]['roc_auc']:.4f}\\n\")\n# print(f\"Classification Report:\\n{model_results[model_name]['classification_report']}\")\n# print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:55:35.2633Z","iopub.status.idle":"2025-02-01T09:55:35.263673Z","shell.execute_reply":"2025-02-01T09:55:35.263503Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8 0%, \n        #d6eaf8 10%, \n        #f2f4f4 10%, \n        #f2f4f4 20%, \n        #ffe5b4 20%, \n        #ffe5b4 30%, \n        #ffffcc 30%, \n        #ffffcc 40%, \n        #d1f2eb 40%, \n        #d1f2eb 50%, \n        #f3e5f5 50%, \n        #f3e5f5 60%, \n        #ffe4e1 60%, \n        #ffe4e1 70%\n    );\n    color: #333;\n    padding: 10px;\n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    border-radius: 10px;\n\">\n    <h2><strong>Stage 16 - Plotting the Train Vs Test Accuracy Chart</strong></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n\n# Initialize a list to store results for all models\nresults_list = []\n\n# Iterate through the models to collect results and plot confusion matrix and ROC curve\nfor model_name, model in model_results.items():\n    # Extract the predicted values and actual values\n    y_test_pred = model['y_test_pred']  # Use the predicted labels\n    y_test = model['y_test']  # Actual true labels\n    \n    # Extract metrics\n    train_accuracy = model['train_accuracy']\n    test_accuracy = model['test_accuracy']\n    roc_auc = model['roc_auc']\n    \n    # Classification Report\n    clf_report = classification_report(y_test, y_test_pred)\n\n    # Print the model name followed by its evaluation metrics\n    print(\"-\" * 40)\n    print(f\"Model: {model_name}\")\n    print(\"-\" * 40)\n    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"ROC AUC: {roc_auc:.4f}\")\n    print(\"Classification Report:\")\n    print(clf_report)\n    print(\"-\" * 80)  # Separator line for clarity\n    \n    # Generate confusion matrix\n    cm = confusion_matrix(y_test, y_test_pred)\n\n    # ROC Curve\n    fpr, tpr, _ = roc_curve(y_test, y_test_pred)\n    roc_auc_value = auc(fpr, tpr)\n\n    # Create subplots: 1 row, 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))  # Width, Height\n\n    # Plot ROC Curve on the first subplot\n    ax1.plot(fpr, tpr, color='b', lw=2, label=f'ROC curve (area = {roc_auc_value:.2f})')\n    ax1.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random classifier line\n    ax1.set_xlim([0.0, 1.0])\n    ax1.set_ylim([0.0, 1.05])\n    ax1.set_xlabel('False Positive Rate')\n    ax1.set_ylabel('True Positive Rate')\n    ax1.set_title(f'ROC Curve for {model_name}')\n    ax1.legend(loc='lower right')\n\n    # Plot Confusion Matrix on the second subplot\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=['Predicted Negative', 'Predicted Positive'],\n                yticklabels=['Actual Negative', 'Actual Positive'], ax=ax2)\n    ax2.set_title(f'Confusion Matrix for {model_name}')\n    ax2.set_xlabel('Predicted')\n    ax2.set_ylabel('Actual')\n\n    # Show both plots\n    plt.tight_layout()\n    plt.show()\n\n    # Append the results to the list for the DataFrame\n    results_list.append({\n        'Model': model_name,\n        'Train Accuracy': f\"{train_accuracy:.4f}\",\n        'Test Accuracy': f\"{test_accuracy:.4f}\",\n        'ROC AUC': f\"{roc_auc:.4f}\",\n        'Classification Report': clf_report\n    })\n\n# Convert results into a DataFrame for better presentation\nresults_df = pd.DataFrame(results_list)\n\n# Print the summary of results in a tabular format\n# print(\"\\nSummary of Model Evaluation Results:\")\n# print(results_df.to_string(index=False))  # Display as a pretty table\nprint(\"-\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:55:35.265042Z","iopub.status.idle":"2025-02-01T09:55:35.265432Z","shell.execute_reply":"2025-02-01T09:55:35.265259Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8, \n        #f2f4f4 20%, \n        #ffe5b4 40%, \n        #ffffcc 60%, \n        #d1f2eb 80%, \n        #f3e5f5 100%\n    );\n    padding: 20px; \n    margin: 20px 0; \n    border-radius: 10px; \n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    font-family: Arial, sans-serif; \n    color: #333;\n    text-align: left;\n\">\n    <h2><strong>Stage 17 - Final Conclusion</strong></h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Conclusion:\n\n1. **High Test Accuracy**: The model achieved a high test accuracy, indicating that it correctly predicted most instances in the test set. This is a promising result for the overall performance of the model.\n   \n2. **ROC AUC**: The **ROC AUC** is nearly 0.5, which is close to random guessing. This suggests that the model struggles to distinguish between the two classes effectively. The low ROC AUC indicates poor discriminative power, especially for class 1.\n\n3. **Class Imbalance**: The classification report highlights a significant class imbalance. \n   - **Class 0** (majority class) has a high precision of **0.95** and recall of **1.00**, with an **F1-score of 0.97**, indicating that the model performs very well on class 0.\n   - **Class 1** (minority class) has very low precision (**0.02**) and recall (**0.00**), with an **F1-score of 0.00**, indicating that the model struggles severely to identify the minority class (class 1).\n   \n4. **Impact of Class Imbalance**: The poor performance on class 1 suggests that the model may be biased towards predicting the majority class (class 0), and thus failing to identify the minority class. This is supported by the low recall and precision for class 1.\n\n5. **Model Improvement Suggestions**:\n   - **Address Class Imbalance**: Techniques such as resampling (SMOTE), class weights adjustment, or using more balanced metrics like **F1-score** for class 1 can help improve the model's ability to detect the minority class.\n   - **Model Tuning**: Exploring other models or hyperparameters to better balance accuracy across both classes may improve performance.\n\n7. **Final Remarks**: While the model shows strong performance in terms of overall accuracy, it is heavily biased towards the majority class, which makes it unreliable for detecting the minority class. Addressing the class imbalance should be a priority for improving model performance in real-world scenarios.","metadata":{}},{"cell_type":"markdown","source":"#### Final Remarks:\n\n1. The model demonstrates strong overall accuracy, indicating its ability to correctly predict the majority of instances within the dataset.\n   \n2. There is a noticeable discrepancy between training and testing accuracy, which may suggest some degree of overfitting, although the difference is not extreme.\n\n3. The ROC AUC score is close to random guessing, indicating that the model struggles with distinguishing between the two classes, especially for the minority class.\n\n4. Class imbalance is a significant issue, as the model shows excellent performance on the majority class but fails to effectively identify the minority class.\n\n5. Precision and recall for the majority class are very high, showcasing that the model can accurately predict this class without many false positives or negatives.\n\n6. The performance for the minority class is poor, with the model having difficulty detecting and correctly predicting instances of this class.\n\n7. The model's inability to perform well on the minority class suggests a bias toward the majority class, which reduces its overall usefulness in cases where detecting the minority class is important.\n\n8. There is an imbalance between the precision and recall of the two classes, with the model being much more sensitive to the majority class.\n\n9. Improvements to the model should focus on addressing class imbalance, such as through resampling techniques, class weighting, or exploring alternative models that are more adept at handling skewed distributions.\n\n10. The current model, while performing well on the majority class, needs further optimization and tuning to ensure it can reliably detect the minority class and be more robust across all categories.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"\n    background: linear-gradient(45deg, \n        #d6eaf8, \n        #f2f4f4 20%, \n        #ffe5b4 40%, \n        #ffffcc 60%, \n        #d1f2eb 80%, \n        #f3e5f5 100%\n    );\n    padding: 20px; \n    margin: 20px 0; \n    border-radius: 10px; \n    box-shadow: 5px 5px 15px rgba(0, 0, 0, 0.3);\n    font-family: Arial, sans-serif; \n    color: #333;\n    text-align: center;\n\">\n    <h2><strong>Stage 18 - Thank You</strong></h2>\n    <p><center><strong>If it didnt make you cry (tears of frustration or boredom), give that vote button a little click! </strong><center></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Thanks a ton for taking the time to dive into the code! If you enjoyed the ride (or at least didnt fall asleep halfway through), dont forget to hit that shiny vote button. \n\nIts like a high-five in the digital worldexcept without the awkward hand placement. So go ahead, show some love, and lets make sure this code gets the recognition it deserves! \n\nHappy coding! And remember, your vote could save a developers day!","metadata":{}}]}